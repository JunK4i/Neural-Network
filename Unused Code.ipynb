{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9a4798",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = merged_df.loc[100]\n",
    "test\n",
    "image = getImagePath(test['image_id'])\n",
    "xyxy = [test['x_1'], test['y_1'], test['width'], test['height']]\n",
    "xyxy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from skimage import io\n",
    "\n",
    "# Replace 'path_to_your_image.jpg' with the actual path to your image file\n",
    "image_path = getImagePath(test['image_id'])\n",
    "image = io.imread(image_path)\n",
    "\n",
    "# Bounding box coordinates and dimensions\n",
    "bbox = xyxy\n",
    "\n",
    "# Create figure and axes\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(image)\n",
    "\n",
    "# Create a Rectangle patch\n",
    "rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=1, edgecolor='r', facecolor='none')\n",
    "\n",
    "# Add the patch to the Axes\n",
    "ax.add_patch(rect)\n",
    "\n",
    "# Set the x and y axis limits to match the image size if needed\n",
    "ax.set_xlim(0, image.shape[1])\n",
    "ax.set_ylim(image.shape[0], 0)  # Y-axis is inverted for images\n",
    "\n",
    "# Show the plot with the bounding box\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebe24a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imgaug.augmenters as iaa\n",
    "from ultralytics import YOLO, SAM\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "def load_image(source_image_path):\n",
    "    image = cv2.imread(source_image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "def get_bounding_box(yolo_model, image):\n",
    "    person_class_index = 0 # ultralytics class dictionary\n",
    "    results= yolo_model.predict(image, conf=0.5, classes=[person_class_index], verbose=False)\n",
    "    for result in results:\n",
    "        if result:\n",
    "            boxes = result.boxes\n",
    "            bbox = boxes.xyxy.tolist()[0]  \n",
    "\n",
    "        else:\n",
    "            height, width, _ = image.shape\n",
    "            bbox = [0, 0, width, height]  \n",
    "    return bbox\n",
    "\n",
    "def apply_segmentation(sam_model, image, bbox):\n",
    "    sam_output = sam_model.predict(image, bboxes=bbox, verbose=False)\n",
    "    mask_object = sam_output[0].masks\n",
    "    mask_tensor = mask_object.data\n",
    "    mask_np = mask_tensor.numpy()\n",
    "    mask_2d = mask_np.squeeze(axis=0)\n",
    "    return mask_2d\n",
    "\n",
    "def apply_mask(image, mask_2d):\n",
    "    masked_image = np.zeros_like(image)\n",
    "    for c in range(image.shape[2]):  # Assuming image has 3 channels\n",
    "        masked_image[..., c] = image[..., c] * mask_2d\n",
    "    return masked_image\n",
    "\n",
    "def crop_and_resize(masked_image, mask_2d, resize_dims):\n",
    "    y_indices, x_indices = np.where(mask_2d)\n",
    "    x_min, x_max = x_indices.min(), x_indices.max()\n",
    "    y_min, y_max = y_indices.min(), y_indices.max()\n",
    "    cropped_image = masked_image[y_min:y_max+1, x_min:x_max+1]\n",
    "    resized_image = cv2.resize(cropped_image, resize_dims)\n",
    "    normalized_image = resized_image / 255.0  # Scale pixel values to [0, 1]\n",
    "    return normalized_image\n",
    "\n",
    "def augment_images(images, augmenter):\n",
    "    augmented_images = []\n",
    "    for img in images:\n",
    "        # Check if image data type is uint8, if not convert to uint8\n",
    "        if img.dtype != np.uint8:\n",
    "            img_uint8 = (img * 255).astype(np.uint8)\n",
    "            augmented = augmenter(image=img_uint8)\n",
    "        else:\n",
    "            augmented = augmenter(image=img)\n",
    "        augmented_images.append(augmented)\n",
    "    return augmented_images\n",
    "\n",
    "def process_batch(batch, yolo_model, sam_model, resize_dims, augmenter, pbar=None):\n",
    "    #batch here is batch_paths\n",
    "    batch_results = []\n",
    "    for source_image_path in batch:\n",
    "        try:\n",
    "            image = load_image(source_image_path)\n",
    "            bbox = get_bounding_box(yolo_model, image)\n",
    "            mask_2d = apply_segmentation(sam_model, image, bbox)\n",
    "            masked_image = apply_mask(image, mask_2d)\n",
    "            normalized_image = crop_and_resize(masked_image, mask_2d, resize_dims)\n",
    "            batch_results.append(normalized_image)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process image {source_image_path}: {e}\")\n",
    "            if pbar is not None:\n",
    "                pbar.update(1)  # Update the progress bar even if there's an error\n",
    "            continue  # Skip the rest of the loop and proceed with the next image\n",
    "        if pbar is not None:\n",
    "            pbar.update(1) \n",
    "    try:\n",
    "        augmented_batch = augment_images(batch_results, augmenter)\n",
    "#         display_batch(augmented_batch)\n",
    "        save_batch_images(batch_results, batch, CELEBA_DATA_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to augment and save images: {e}\")\n",
    "    return augmented_batch\n",
    "\n",
    "def display_batch(images, figsize=(8, 8), columns=5):\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(len(images) // columns + 1, columns, i + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.text(5, 15, str(i), color='white', fontsize=12, weight='bold')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()    \n",
    "\n",
    "def process_partition(df_partition, yolo_model, sam_model, resize_dims, augmenter, batch_size=32):\n",
    "    num_images = len(df_partition)\n",
    "    processed_images = []\n",
    "    with tqdm(total=num_images, desc=\"Processing partition\", unit=\"img\") as pbar:\n",
    "        for start_idx in range(0, num_images, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, num_images)\n",
    "            # get image paths from the partitioned_df\n",
    "            batch_paths = [getImagePath(df_partition.loc[i]['image_id']) for i in range(start_idx, end_idx)]\n",
    "            processed_batch = process_batch(batch_paths, yolo_model, sam_model, resize_dims, augmenter, pbar)\n",
    "            processed_images.extend(processed_batch)\n",
    "    return processed_images\n",
    "\n",
    "def save_batch_images(batch_images, batch_image_paths, base_output_dir):\n",
    "    \"\"\"\n",
    "    Save a batch of images to the specified base output directory.\n",
    "\n",
    "    :param batch_images: List of image data to be saved.\n",
    "    :param batch_image_paths: List of source paths of images, used to extract the image IDs.\n",
    "    :param base_output_dir: Base directory where the 'processed_img' folder will be created and images will be saved.\n",
    "    \"\"\"\n",
    "    # Create the output directory if it does not exist\n",
    "    output_dir = os.path.join(base_output_dir, 'processed_img')\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Save each image in the batch\n",
    "    for img, path in zip(batch_images, batch_image_paths):\n",
    "        image_id = os.path.basename(path)\n",
    "        output_image_path = os.path.join(output_dir, image_id)\n",
    "        img_pil = Image.fromarray((img * 255).astype('uint8'))  # Convert from [0,1] to [0,255] and to uint8\n",
    "        img_pil.save(output_image_path, format=\"JPEG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da01027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils.torch_utils import select_device\n",
    "\n",
    "#change to gpu or empty string to auto-select first avail\n",
    "selected_device = select_device(device='mps', batch=32, newline=False, verbose=True) \n",
    "print(f\"Using device: {selected_device}\")\n",
    "\n",
    "# Initialize models\n",
    "yolo_model = YOLO('yolov8s.pt')\n",
    "sam_model = SAM('mobile_sam.pt')\n",
    "\n",
    "\n",
    "augmenter = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5),  # Horizontal flips\n",
    "    iaa.Crop(percent=(0, 0.1)),  # Random crops\n",
    "    iaa.Sometimes(0.5, iaa.GaussianBlur(sigma=(0, 0.5))),\n",
    "    iaa.ContrastNormalization((0.75, 1.5)),\n",
    "    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "    iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "    iaa.Affine(\n",
    "        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "        rotate=(-25, 25),\n",
    "        shear=(-8, 8)\n",
    "    )\n",
    "], random_order=True)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "resize_dims = (224, 224)\n",
    "train_processed = process_partition(train_df, yolo_model, sam_model, resize_dims, augmenter)\n",
    "train_processed = process_partition(val_df, yolo_model, sam_model, resize_dims, augmenter)\n",
    "train_processed = process_partition(test_df, yolo_model, sam_model, resize_dims, augmenter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5012503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "\n",
    "def calculate_weights_to_tensor(merged_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(merged_path)\n",
    "\n",
    "    # Get class counts for the 'Male' attribute\n",
    "    male_class_counts = df['Male'].value_counts().sort_index()\n",
    "\n",
    "    # Alternatively, compute class weights using sklearn's utility function\n",
    "    male_class_weights_sklearn = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=[0, 1],\n",
    "        y=df['Male'].values\n",
    "    )\n",
    "    \n",
    "    # Store the class weights in a dictionary\n",
    "    class_weights_dict = {'Male': {0: male_class_weights_sklearn[0], 1: male_class_weights_sklearn[1]}}\n",
    "\n",
    "    # first two columns are 'img_id', 'male', 'Partition', we skip them\n",
    "    # The rest are the attributes you want to calculate class weights for\n",
    "    attribute_columns = df.columns[3:]\n",
    "\n",
    "    # Iterate over the attribute columns to calculate class weights\n",
    "    for attribute in attribute_columns:\n",
    "        # Compute class weights for the current attribute\n",
    "        class_weights = compute_class_weight(\n",
    "            class_weight='balanced', \n",
    "            classes=[0, 1], \n",
    "            y=df[attribute].values\n",
    "        )\n",
    "        \n",
    "        # Store the weights in the dictionary\n",
    "        class_weights_dict[attribute] = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "    # Print the class weights\n",
    "    for attribute, weights in class_weights_dict.items():\n",
    "        print(f\"Class weights for {attribute}: {weights}\\n\")\n",
    "    \n",
    "    # Convert the class weights dictionary to a tensor\n",
    "    weights_tensor = torch.tensor([list(weights.values()) for _, weights in class_weights_dict.items()])\n",
    "\n",
    "    # This tensor will be 2D with shape [num_attributes, 2], where the second dimension contains the weights for classes 0 and 1\n",
    "    return weights_tensor\n",
    "\n",
    "# Usage example:\n",
    "weights_tensor = calculate_weights_to_tensor(merged_path)\n",
    "print(weights_tensor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
