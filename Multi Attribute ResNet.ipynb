{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c32817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet50\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60706c30-79fd-43a7-83c8-684a18308635",
   "metadata": {},
   "outputs": [],
   "source": [
    "CELEBA_DATA_PATH = '../'\n",
    "IMG_PATH = os.path.join(CELEBA_DATA_PATH, 'img_align_celeba')\n",
    "ATTR_PATH = os.path.join(CELEBA_DATA_PATH, 'list_attr_celeba.csv')\n",
    "PARTITION_PATH = os.path.join(CELEBA_DATA_PATH, 'partitioned.csv')\n",
    "merged_path = \"../partitioned_multi_attr.csv\"\n",
    "\n",
    "def getImagePath(image_id):\n",
    "    return os.path.join(IMG_PATH,image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a75faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data paths\n",
    "# CELEBA_DATA_PATH = './Data/celeba'\n",
    "# IMG_PATH = os.path.join(CELEBA_DATA_PATH, 'img_align_celeba/img_align_celeba')\n",
    "# ATTR_PATH = os.path.join(CELEBA_DATA_PATH, 'list_attr_celeba.csv')\n",
    "# PARTITION_PATH = os.path.join(CELEBA_DATA_PATH, 'list_eval_partition.csv')\n",
    "# merged_path = \"./Data/celeba/partitioned_multi_attr.csv\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the data|\n",
    "attributes_df = pd.read_csv(ATTR_PATH)\n",
    "partitioned_df = pd.read_csv(PARTITION_PATH)\n",
    "\n",
    "# Calculate and sort the correlations\n",
    "correlations = attributes_df.drop(columns=['image_id']).corrwith(attributes_df['Male']).abs().sort_values(ascending=False)\n",
    "\n",
    "# # Select attributes with high correlation and exclude subjective ones\n",
    "# selected_attributes = correlations[correlations > 0.2].index.difference(['Attractive', 'Chubby', 'High_Cheekbones'])\n",
    "\n",
    "# # Merge the DataFrames\n",
    "# merged_df = pd.merge(partitioned_df, attributes_df[['image_id'] + selected_attributes.tolist()], on='image_id')\n",
    "\n",
    "# # Convert to 0 and 1\n",
    "# merged_df[merged_df.select_dtypes(include=['number']).columns] = merged_df.select_dtypes(include=['number']).clip(lower=0)\n",
    "\n",
    "# male_column = merged_df.pop('Male')  # remove Male column and store it\n",
    "# merged_df.insert(1, 'Male', male_column)  # insert Male column after image_id\n",
    "\n",
    "# # Export \n",
    "# merged_df.to_csv(\"./partitioned_multi_attr.csv\", index=False)\n",
    "\n",
    "# # Display the first rows of the merged DataFrame\n",
    "# merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "028ff7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, file_paths, file_to_label, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.file_to_label = file_to_label\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.file_paths[idx]\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label = self.file_to_label[os.path.basename(img_name)][0]  # Only the \"Male\" label\n",
    "        attributes = self.file_to_label[os.path.basename(img_name)][2:]  # After partition rest are attributes\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, torch.tensor(attributes, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    \n",
    "df = pd.read_csv(\"../partitioned_multi_attr.csv\")\n",
    "train_df = df[df['partition'] == 0]\n",
    "val_df = df[df['partition'] == 1]\n",
    "test_df = df[df['partition'] == 2]\n",
    "\n",
    "df_labels = df.set_index('image_id')\n",
    "filename_to_label = {filename: labels.values for filename, labels in df_labels.iterrows()}\n",
    "file_paths = df['image_id'].apply(getImagePath).tolist()\n",
    "\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbbee6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the images are in a directory named 'images' in the current working directory\n",
    "# Create separate file path and label mappings for each dataset partition\n",
    "train_file_paths = train_df['image_id'].apply(getImagePath).tolist()\n",
    "val_file_paths = val_df['image_id'].apply(getImagePath).tolist()\n",
    "test_file_paths = test_df['image_id'].apply(getImagePath).tolist()\n",
    "\n",
    "train_filename_to_label = {filename: labels.values for filename, labels in train_df.set_index('image_id').iterrows()}\n",
    "val_filename_to_label = {filename: labels.values for filename, labels in val_df.set_index('image_id').iterrows()}\n",
    "test_filename_to_label = {filename: labels.values for filename, labels in test_df.set_index('image_id').iterrows()}\n",
    "\n",
    "# Initialize the datasets for each partition\n",
    "train_dataset = CelebADataset(train_file_paths, train_filename_to_label, transform=transform)\n",
    "val_dataset = CelebADataset(val_file_paths, val_filename_to_label, transform=transform)\n",
    "test_dataset = CelebADataset(test_file_paths, test_filename_to_label, transform=transform)\n",
    "\n",
    "# Create data loaders for each dataset partition\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73bea6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hunts\\anaconda3\\envs\\NN_Proj\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hunts\\anaconda3\\envs\\NN_Proj\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained ResNet model\n",
    "model = resnet50(pretrained=True)\n",
    "\n",
    "# Modify the model for binary classification\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)  # 2 classes: Male/Female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "697e26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load a pre-trained ResNet model\n",
    "model = resnet50(pretrained=True)\n",
    "\n",
    "# Assume the number of attributes is equal to the number of columns minus 3 (for the image_id, male, partition)\n",
    "num_attributes = df.shape[1] - 3\n",
    "\n",
    "# Modify the model for binary classification plus additional attributes\n",
    "class MultiInputResNet(nn.Module):\n",
    "    def __init__(self, base_model, num_attributes, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        num_ftrs = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Identity()  # Remove the original fully connected layer\n",
    "\n",
    "        # Fully connected layer for attributes\n",
    "        self.attr_fc = nn.Linear(num_attributes, 224)  \n",
    "\n",
    "        # Final fully connected layer that takes both image features and attributes\n",
    "        self.final_fc = nn.Linear(num_ftrs + 224, 1)  # Modify num_classes based on your classification problem\n",
    "\n",
    "    def forward(self, image, attributes):\n",
    "        # Get image features from the ResNet\n",
    "        img_features = self.base_model(image)\n",
    "\n",
    "        # Process attributes\n",
    "        attr_features = self.attr_fc(attributes)\n",
    "\n",
    "        # Concatenate image and attribute features\n",
    "        combined_features = torch.cat((img_features, attr_features), dim=1)\n",
    "\n",
    "        # Final classification layer\n",
    "        return self.final_fc(combined_features)\n",
    "\n",
    "# Update the model\n",
    "multi_attr_model = MultiInputResNet(base_model=model, num_attributes=num_attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5e700fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=5, verbose=False, delta=0, checkpoint_name=\"muti_attr_checkpoint.pt\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            checkpoint_name (str): Name of the checkpoint file. \n",
    "                            Default: \"checkpoint.pt\"\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "        self.checkpoint_name = checkpoint_name \n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Saves model when validation loss decreases.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.checkpoint_name) \n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "        \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, checkpoint_name):\n",
    "    # Initialize early stopping\n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True, checkpoint_name=checkpoint_name)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_pbar = tqdm(train_loader, unit=\"batch\")\n",
    "\n",
    "        for images, labels, attributes in train_pbar:\n",
    "            train_pbar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            images, labels, attributes = images.to(device), labels.to(device).float(), attributes.to(device).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, attributes)\n",
    "            \n",
    "            labels = labels.view(-1, 1) if labels.ndim == 1 else labels\n",
    "            \n",
    "            loss = criterion(outputs, labels)  \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            train_pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_pbar = tqdm(val_loader, unit=\"batch\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels, attributes in val_pbar:\n",
    "                val_pbar.set_description(f\"Val Epoch {epoch+1}/{num_epochs}\")\n",
    "                images, labels, attributes = images.to(device), labels.to(device).float(), attributes.to(device).float()\n",
    "                \n",
    "                outputs = model(images, attributes)\n",
    "                \n",
    "                labels = labels.view(-1, 1) if labels.ndim == 1 else labels\n",
    "                \n",
    "                loss = criterion(outputs, labels)  \n",
    "                val_running_loss += loss.item() * images.size(0)\n",
    "                val_pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} Train loss: {train_loss:.4f} Val loss: {val_loss:.4f}')\n",
    "\n",
    "        # Call early stopping\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    model.load_state_dict(torch.load(checkpoint_name))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "826781fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████████████████████████████████████████████| 1250/1250 [02:01<00:00, 10.33batch/s, loss=0.0544]\n",
      "Val Epoch 1/20: 100%|████████████████████████████████████████████████| 157/157 [00:08<00:00, 17.95batch/s, loss=0.0461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Train loss: 0.0770 Val loss: 0.0779\n",
      "Validation loss decreased (inf --> 0.077858).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|███████████████████████████████████████████████████| 1250/1250 [02:01<00:00, 10.32batch/s, loss=0.104]\n",
      "Val Epoch 2/20: 100%|████████████████████████████████████████████████| 157/157 [00:08<00:00, 17.74batch/s, loss=0.0276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 Train loss: 0.0634 Val loss: 0.0664\n",
      "Validation loss decreased (0.077858 --> 0.066408).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████████████████████████████████████████████| 1250/1250 [02:00<00:00, 10.36batch/s, loss=0.0216]\n",
      "Val Epoch 3/20: 100%|████████████████████████████████████████████████| 157/157 [00:08<00:00, 17.84batch/s, loss=0.0208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 Train loss: 0.0555 Val loss: 0.0811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|███████████████████████████████████████████████████| 1250/1250 [02:00<00:00, 10.35batch/s, loss=0.031]\n",
      "Val Epoch 4/20: 100%|███████████████████████████████████████████████| 157/157 [00:08<00:00, 17.84batch/s, loss=0.00873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 Train loss: 0.0469 Val loss: 0.0687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████████████████████████████████████████████| 1250/1250 [02:01<00:00, 10.29batch/s, loss=0.0485]\n",
      "Val Epoch 5/20: 100%|████████████████████████████████████████████████| 157/157 [00:08<00:00, 17.98batch/s, loss=0.0245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 Train loss: 0.0405 Val loss: 0.0792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████████████████████████████████████████████| 1250/1250 [02:01<00:00, 10.33batch/s, loss=0.0223]\n",
      "Val Epoch 6/20: 100%|███████████████████████████████████████████████| 157/157 [00:08<00:00, 17.91batch/s, loss=0.00668]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 Train loss: 0.0356 Val loss: 0.0726\n",
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|█████████████████████████████████████████████████| 1250/1250 [02:00<00:00, 10.37batch/s, loss=0.00591]\n",
      "Val Epoch 7/20: 100%|███████████████████████████████████████████████| 157/157 [00:08<00:00, 17.89batch/s, loss=0.00341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 Train loss: 0.0153 Val loss: 0.0653\n",
      "Validation loss decreased (0.066408 --> 0.065276).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████████████████████████████████████████████| 1250/1250 [02:01<00:00, 10.30batch/s, loss=0.0638]\n",
      "Val Epoch 8/20: 100%|███████████████████████████████████████████████| 157/157 [00:08<00:00, 17.93batch/s, loss=0.00433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 Train loss: 0.0069 Val loss: 0.0724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|█████████████████████████████████████████████████| 1250/1250 [02:01<00:00, 10.32batch/s, loss=0.00177]\n",
      "Val Epoch 9/20: 100%|███████████████████████████████████████████████| 157/157 [00:08<00:00, 17.79batch/s, loss=0.00246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 Train loss: 0.0030 Val loss: 0.0815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|███████████████████████████████████████████████| 1250/1250 [02:00<00:00, 10.35batch/s, loss=0.000289]\n",
      "Val Epoch 10/20: 100%|██████████████████████████████████████████████| 157/157 [00:08<00:00, 17.87batch/s, loss=0.00118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 Train loss: 0.0020 Val loss: 0.0860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|████████████████████████████████████████████████| 1250/1250 [01:53<00:00, 11.04batch/s, loss=9.83e-5]\n",
      "Val Epoch 11/20: 100%|██████████████████████████████████████████████| 157/157 [00:08<00:00, 19.54batch/s, loss=0.00199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 Train loss: 0.0011 Val loss: 0.0975\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|████████████████████████████████████████████████| 1250/1250 [01:43<00:00, 12.05batch/s, loss=8.93e-5]\n",
      "Val Epoch 12/20: 100%|██████████████████████████████████████████████| 157/157 [00:08<00:00, 19.58batch/s, loss=0.00142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 Train loss: 0.0007 Val loss: 0.0995\n",
      "Early stopping\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torchvision.models import resnet50\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming the `MultiInputResNet` class has been defined as discussed previously\n",
    "\n",
    "# Check if GPU is available and move the model to GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "multi_attr_model.to(device)\n",
    "\n",
    "# Assuming train_loader and val_loader have been defined and are ready to use\n",
    "# Define the loss function and optimizer\n",
    "criterion = BCEWithLogitsLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1, verbose=True)\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 20  # Change this to your desired number of epochs\n",
    "\n",
    "# Run the training function\n",
    "trained_model = train_model(\n",
    "    model=multi_attr_model, \n",
    "    train_loader=train_loader, \n",
    "    val_loader=val_loader, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer, \n",
    "    scheduler=scheduler, \n",
    "    num_epochs=num_epochs, \n",
    "    device=device, \n",
    "    checkpoint_name=\"multi_attr_checkpoint.pt\"\n",
    ")\n",
    "\n",
    "# Save the trained model state\n",
    "torch.save(trained_model.state_dict(), \"multi_attr_model_final.pt\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4bca599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|███████████████████████████████████████████████████████████████████████| 157/157 [00:08<00:00, 18.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0020\n",
      "Accuracy: 0.9808\n",
      "Precision: 0.9779\n",
      "Recall: 0.9760\n",
      "F1 Score: 0.9770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def test_model(model, dataloader, device, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images,labels, attributes  in tqdm(dataloader, desc=\"Testing\"):\n",
    "            # Move data to the appropriate device\n",
    "            images = images.to(device).float() \n",
    "            attributes = attributes.to(device).float()  \n",
    "            labels = labels.to(device).float() \n",
    "            # Forward pass to get the logits\n",
    "            outputs = model(images, attributes).squeeze()\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Convert logits to probabilities\n",
    "            probs = torch.sigmoid(outputs).squeeze()\n",
    "\n",
    "            # Convert probabilities to predicted class (0 or 1)\n",
    "            predictions = (probs >= 0.5).int()\n",
    "\n",
    "            # Store true labels and predictions for later evaluation\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predictions.cpu().numpy())\n",
    "\n",
    "    # Calculate the average loss\n",
    "    test_loss /= len(dataloader.dataset)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.4f}')\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "    return test_loss, accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "state_dict = torch.load(\"multi_attr_model_final.pt\", map_location=device)\n",
    "multi_attr_model.load_state_dict(state_dict)\n",
    "multi_attr_model.to(device)\n",
    "\n",
    "\n",
    "# Assuming that the model has been trained and the state_dict has been loaded\n",
    "# and that test_loader has been defined\n",
    "test_loss, accuracy, precision, recall, f1 = test_model(\n",
    "    model=multi_attr_model,\n",
    "    dataloader=test_loader,\n",
    "    device=device,\n",
    "    criterion=criterion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4444f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe6060b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
