{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e18be072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n",
      "/device:GPU:0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-09 13:12:04.457496: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2023-11-09 13:12:04.457518: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2023-11-09 13:12:04.457521: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2023-11-09 13:12:04.457553: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-11-09 13:12:04.457565: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm.notebook import tqdm\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# mac optimization codes\n",
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.test.gpu_device_name())\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fae0898a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Data/celeba/partitioned.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>partition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39087</th>\n",
       "      <td>039088.jpg</td>\n",
       "      <td>Female</td>\n",
       "      <td>Old</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30893</th>\n",
       "      <td>030894.jpg</td>\n",
       "      <td>Male</td>\n",
       "      <td>Old</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45278</th>\n",
       "      <td>045279.jpg</td>\n",
       "      <td>Female</td>\n",
       "      <td>Young</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16398</th>\n",
       "      <td>016399.jpg</td>\n",
       "      <td>Female</td>\n",
       "      <td>Young</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13653</th>\n",
       "      <td>013654.jpg</td>\n",
       "      <td>Male</td>\n",
       "      <td>Old</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  Gender    Age  partition\n",
       "39087  039088.jpg  Female    Old          0\n",
       "30893  030894.jpg    Male    Old          0\n",
       "45278  045279.jpg  Female  Young          0\n",
       "16398  016399.jpg  Female  Young          0\n",
       "13653  013654.jpg    Male    Old          0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the Dataset\n",
    "\n",
    "CELEBA_DATA_PATH = './Data/celeba'\n",
    "IMG_PATH = os.path.join(CELEBA_DATA_PATH, 'img_align_celeba/img_align_celeba')\n",
    "CROPPED_IMG_PATH = os.path.join(CELEBA_DATA_PATH, 'processed_img/')\n",
    "\n",
    "ATTR_PATH = os.path.join(CELEBA_DATA_PATH,'list_attr_celeba.csv') #for gender and age\n",
    "\n",
    "\n",
    "def getImagePath(image_id):\n",
    "    return os.path.join(IMG_PATH,image_id)\n",
    "\n",
    "def getCroppedPath(image_id):\n",
    "    return os.path.join(CROPPED_IMG_PATH,image_id)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the attributes\n",
    "attributes_df = pd.read_csv(ATTR_PATH)\n",
    "attributes_df['Gender'] = attributes_df['Male'].map({1: 'Male', -1: 'Female'})\n",
    "attributes_df['Age'] = attributes_df['Young'].map({1: 'Young', -1: 'Old'})\n",
    "attributes_df = attributes_df[['image_id', 'Gender', 'Age']]\n",
    "\n",
    "# Get first 50k\n",
    "cropped_images_df = attributes_df.head(50000)\n",
    "\n",
    "# Split the data into training and validation sets (80-20 split for example)\n",
    "train_df, val_test_df = train_test_split(cropped_images_df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Assign partition labels: 0 for train and 1 for validation\n",
    "train_df['partition'] = 0\n",
    "val_df['partition'] = 1\n",
    "test_df['partition'] = 2\n",
    "\n",
    "# Combine back to a single dataframe\n",
    "partitioned_df = pd.concat([train_df, val_df, test_df])\n",
    "\n",
    "PARTITION_OUTPUT_PATH = os.path.join(CELEBA_DATA_PATH,\"partitioned.csv\")\n",
    "\n",
    "# Export the partition data to a new CSV file\n",
    "try:\n",
    "    print(PARTITION_OUTPUT_PATH)\n",
    "    partitioned_df.to_csv(PARTITION_OUTPUT_PATH, index=False)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# The 'new_partition.csv' file will now have the image_id, Gender, Age, and partition columns\n",
    "partitioned_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7f89bda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# Define a custom dataset\n",
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.dataframe.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "        # Assuming the 'Gender' column is the second column and contains numeric values after mapping\n",
    "        label = self.dataframe.iloc[idx, 1]\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Load the partitioned dataset\n",
    "df = pd.read_csv(PARTITION_OUTPUT_PATH)\n",
    "\n",
    "# Assign binary labels to the 'Gender' column\n",
    "df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n",
    "\n",
    "# Split the dataframe into training and validation sets\n",
    "train_df = df[df['partition'] == 0]\n",
    "val_df = df[df['partition'] == 1]\n",
    "test_df = df[df['partition'] == 2]\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Function to create a data loader given the image paths\n",
    "def create_data_loader(df, img_dir, transform, batch_size=32):\n",
    "    dataset = CelebADataset(dataframe=df, img_dir=img_dir, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return loader\n",
    "\n",
    "# Assuming IMG_PATH and CROPPED_IMG_PATH are defined\n",
    "train_loader = create_data_loader(train_df, IMG_PATH, transform)\n",
    "val_loader = create_data_loader(val_df, IMG_PATH, transform)\n",
    "test_loader = create_data_loader(test_df, IMG_PATH, transform)\n",
    "\n",
    "cropped_train_loader = create_data_loader(train_df, CROPPED_IMG_PATH, transform)\n",
    "cropped_val_loader = create_data_loader(val_df, CROPPED_IMG_PATH, transform)\n",
    "cropped_test_loader = create_data_loader(test_df, CROPPED_IMG_PATH, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "60779045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 1) \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 28 * 28)  # Flatten the layer\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a59dba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, checkpoint_name=\"checkpoint.pt\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            checkpoint_name (str): Name of the checkpoint file. \n",
    "                            Default: \"checkpoint.pt\"\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "        self.checkpoint_name = checkpoint_name \n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Saves model when validation loss decreases.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.checkpoint_name) \n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "        \n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, checkpoint_name):\n",
    "    # Initialize early stopping\n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True, checkpoint_name=checkpoint_name)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_pbar = tqdm(train_loader, unit=\"batch\")\n",
    "        for inputs, labels in train_pbar:\n",
    "            train_pbar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.view(-1, 1))  \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            train_pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_pbar = tqdm(val_loader, unit=\"batch\")\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_pbar:\n",
    "                val_pbar.set_description(f\"Val Epoch {epoch+1}/{num_epochs}\")\n",
    "                inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels.view(-1, 1))  \n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                val_pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} Train loss: {train_loss:.4f} Val loss: {val_loss:.4f}')\n",
    "\n",
    "        # Call early stopping\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    # Load the last checkpoint with the best model\n",
    "    model.load_state_dict(torch.load(checkpoint_name))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0c7de9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   4%|â–Œ              | 50/1250 [00:06<02:26,  8.20batch/s, loss=0.415]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Call the training function for non-cropped images\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m trained_model_non_cropped \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_non_cropped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_non_cropped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_non_cropped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_non_cropped_checkpoint.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     32\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Call the training function for cropped images\u001b[39;00m\n\u001b[1;32m     35\u001b[0m trained_model_cropped \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[1;32m     36\u001b[0m     model_cropped,\n\u001b[1;32m     37\u001b[0m     cropped_train_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     checkpoint_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_cropped_checkpoint.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     45\u001b[0m )\n",
      "Cell \u001b[0;32mIn[88], line 65\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, checkpoint_name)\u001b[0m\n\u001b[1;32m     63\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     64\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 65\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     66\u001b[0m     train_pbar\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     68\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch import nn\n",
    "\n",
    "# For non-cropped images\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "model_non_cropped = SimpleCNN().to(device)\n",
    "# model_non_cropped = SimpleCNN().to(mps_device)\n",
    "optimizer_non_cropped = torch.optim.Adam(model_non_cropped.parameters(), lr=0.001)\n",
    "scheduler_non_cropped = ReduceLROnPlateau(optimizer_non_cropped, 'min', patience=3, verbose=True)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# For cropped images\n",
    "model_cropped = SimpleCNN().to(device)\n",
    "optimizer_cropped = torch.optim.Adam(model_cropped.parameters(), lr=0.001)\n",
    "scheduler_cropped = ReduceLROnPlateau(optimizer_cropped, 'min', patience=3, verbose=True)\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 1\n",
    "\n",
    "# Call the training function for non-cropped images\n",
    "trained_model_non_cropped = train_model(\n",
    "    model_non_cropped,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer_non_cropped,\n",
    "    scheduler_non_cropped,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    checkpoint_name='model_non_cropped_checkpoint.pt'\n",
    ")\n",
    "\n",
    "# Call the training function for cropped images\n",
    "trained_model_cropped = train_model(\n",
    "    model_cropped,\n",
    "    cropped_train_loader,\n",
    "    cropped_val_loader,\n",
    "    criterion,\n",
    "    optimizer_cropped,\n",
    "    scheduler_cropped,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    checkpoint_name='model_cropped_checkpoint.pt'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5052d717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 61.44%\n",
      "Test Accuracy: 91.18%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():  # No need to track the gradients\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).float()  \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Apply sigmoid and round to get the binary class\n",
    "            predictions = torch.sigmoid(outputs).round().cpu().numpy()\n",
    "            y_pred.extend(predictions)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return accuracy, y_true, y_pred\n",
    "\n",
    "checkpoint = 'model_non_cropped_checkpoint.pt'  \n",
    "model = SimpleCNN().to(device)\n",
    "model.load_state_dict(torch.load(checkpoint))\n",
    "\n",
    "checkpoint = 'model_cropped_checkpoint.pt'  \n",
    "cropped_model = SimpleCNN().to(device)\n",
    "cropped_model.load_state_dict(torch.load(checkpoint))\n",
    "\n",
    "# test both against cropped\n",
    "accuracy, true_labels, predicted_labels = evaluate_model(model, cropped_test_loader, device)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "accuracy, true_labels, predicted_labels = evaluate_model(cropped_model, cropped_test_loader, device)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09084b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400eea4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611d676f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
