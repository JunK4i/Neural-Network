{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c32817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet50\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2a75faf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>Male</th>\n",
       "      <th>partition</th>\n",
       "      <th>5_o_Clock_Shadow</th>\n",
       "      <th>Arched_Eyebrows</th>\n",
       "      <th>Bags_Under_Eyes</th>\n",
       "      <th>Big_Nose</th>\n",
       "      <th>Blond_Hair</th>\n",
       "      <th>Bushy_Eyebrows</th>\n",
       "      <th>Double_Chin</th>\n",
       "      <th>...</th>\n",
       "      <th>No_Beard</th>\n",
       "      <th>Pointy_Nose</th>\n",
       "      <th>Rosy_Cheeks</th>\n",
       "      <th>Sideburns</th>\n",
       "      <th>Wavy_Hair</th>\n",
       "      <th>Wearing_Earrings</th>\n",
       "      <th>Wearing_Lipstick</th>\n",
       "      <th>Wearing_Necklace</th>\n",
       "      <th>Wearing_Necktie</th>\n",
       "      <th>Young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000003.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000004.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000005.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_id  Male  partition  5_o_Clock_Shadow  Arched_Eyebrows  \\\n",
       "0  000001.jpg     0          0                 0                1   \n",
       "1  000002.jpg     0          0                 0                0   \n",
       "2  000003.jpg     1          0                 0                0   \n",
       "3  000004.jpg     0          0                 0                0   \n",
       "4  000005.jpg     0          0                 0                1   \n",
       "\n",
       "   Bags_Under_Eyes  Big_Nose  Blond_Hair  Bushy_Eyebrows  Double_Chin  ...  \\\n",
       "0                0         0           0               0            0  ...   \n",
       "1                1         1           0               0            0  ...   \n",
       "2                0         0           0               0            0  ...   \n",
       "3                0         0           0               0            0  ...   \n",
       "4                0         0           0               0            0  ...   \n",
       "\n",
       "   No_Beard  Pointy_Nose  Rosy_Cheeks  Sideburns  Wavy_Hair  Wearing_Earrings  \\\n",
       "0         1            1            0          0          0                 1   \n",
       "1         1            0            0          0          0                 0   \n",
       "2         1            1            0          0          1                 0   \n",
       "3         1            1            0          0          0                 1   \n",
       "4         1            1            0          0          0                 0   \n",
       "\n",
       "   Wearing_Lipstick  Wearing_Necklace  Wearing_Necktie  Young  \n",
       "0                 1                 0                0      1  \n",
       "1                 0                 0                0      1  \n",
       "2                 0                 0                0      1  \n",
       "3                 1                 1                0      1  \n",
       "4                 1                 0                0      1  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the data paths\n",
    "CELEBA_DATA_PATH = './Data/celeba'\n",
    "IMG_PATH = os.path.join(CELEBA_DATA_PATH, 'img_align_celeba/img_align_celeba')\n",
    "ATTR_PATH = os.path.join(CELEBA_DATA_PATH, 'list_attr_celeba.csv')\n",
    "PARTITION_PATH = os.path.join(CELEBA_DATA_PATH, 'list_eval_partition.csv')\n",
    "merged_path = \"./Data/celeba/partitioned_multi_attr.csv\"\n",
    "\n",
    "\n",
    "def getImagePath(image_id):\n",
    "    return os.path.join(IMG_PATH,image_id)\n",
    "\n",
    "# Load the data|\n",
    "attributes_df = pd.read_csv(ATTR_PATH)\n",
    "partitioned_df = pd.read_csv(PARTITION_PATH)\n",
    "\n",
    "# Calculate and sort the correlations\n",
    "correlations = attributes_df.drop(columns=['image_id']).corrwith(attributes_df['Male']).abs().sort_values(ascending=False)\n",
    "\n",
    "# Select attributes with high correlation and exclude subjective ones\n",
    "selected_attributes = correlations[correlations > 0.2].index.difference(['Attractive', 'Chubby', 'High_Cheekbones'])\n",
    "\n",
    "# Merge the DataFrames\n",
    "merged_df = pd.merge(partitioned_df, attributes_df[['image_id'] + selected_attributes.tolist()], on='image_id')\n",
    "\n",
    "# Convert to 0 and 1\n",
    "merged_df[merged_df.select_dtypes(include=['number']).columns] = merged_df.select_dtypes(include=['number']).clip(lower=0)\n",
    "\n",
    "male_column = merged_df.pop('Male')  # remove Male column and store it\n",
    "merged_df.insert(1, 'Male', male_column)  # insert Male column after image_id\n",
    "\n",
    "# Export \n",
    "merged_df.to_csv(\"./Data/celeba/partitioned_multi_attr.csv\", index=False)\n",
    "\n",
    "# Display the first rows of the merged DataFrame\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "028ff7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, file_paths, file_to_label, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.file_to_label = file_to_label\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.file_paths[idx]\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label = self.file_to_label[os.path.basename(img_name)][0]  # Only the \"Male\" label\n",
    "        attributes = self.file_to_label[os.path.basename(img_name)][2:]  # After partition rest are attributes\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, torch.tensor(attributes, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    \n",
    "df = pd.read_csv(\"./Data/celeba/partitioned_multi_attr.csv\")\n",
    "train_df = df[df['partition'] == 0]\n",
    "val_df = df[df['partition'] == 1]\n",
    "test_df = df[df['partition'] == 2]\n",
    "\n",
    "df_labels = df.set_index('image_id')\n",
    "filename_to_label = {filename: labels.values for filename, labels in df_labels.iterrows()}\n",
    "file_paths = df['image_id'].apply(getImagePath).tolist()\n",
    "\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# train_dataset = CelebADataset(file_paths, filename_to_label, transform=transform)\n",
    "# val_dataset = CelebADataset\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bbbee6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the images are in a directory named 'images' in the current working directory\n",
    "# Create separate file path and label mappings for each dataset partition\n",
    "train_file_paths = train_df['image_id'].apply(getImagePath).tolist()\n",
    "val_file_paths = val_df['image_id'].apply(getImagePath).tolist()\n",
    "test_file_paths = test_df['image_id'].apply(getImagePath).tolist()\n",
    "\n",
    "train_filename_to_label = {filename: labels.values for filename, labels in train_df.set_index('image_id').iterrows()}\n",
    "val_filename_to_label = {filename: labels.values for filename, labels in val_df.set_index('image_id').iterrows()}\n",
    "test_filename_to_label = {filename: labels.values for filename, labels in test_df.set_index('image_id').iterrows()}\n",
    "\n",
    "# Initialize the datasets for each partition\n",
    "train_dataset = CelebADataset(train_file_paths, train_filename_to_label, transform=transform)\n",
    "val_dataset = CelebADataset(val_file_paths, val_filename_to_label, transform=transform)\n",
    "test_dataset = CelebADataset(test_file_paths, test_filename_to_label, transform=transform)\n",
    "\n",
    "# Create data loaders for each dataset partition\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "73bea6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kohjunkai/anaconda3/envs/gender_classification/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/kohjunkai/anaconda3/envs/gender_classification/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained ResNet model\n",
    "model = resnet50(pretrained=True)\n",
    "\n",
    "# Modify the model for binary classification\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)  # 2 classes: Male/Female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "697e26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load a pre-trained ResNet model\n",
    "model = resnet50(pretrained=True)\n",
    "\n",
    "# Assume the number of attributes is equal to the number of columns minus 3 (for the image_id, male, partition)\n",
    "num_attributes = merged_df.shape[1] - 3\n",
    "\n",
    "# Modify the model for binary classification plus additional attributes\n",
    "class MultiInputResNet(nn.Module):\n",
    "    def __init__(self, base_model, num_attributes, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        num_ftrs = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Identity()  # Remove the original fully connected layer\n",
    "\n",
    "        # Fully connected layer for attributes\n",
    "        self.attr_fc = nn.Linear(num_attributes, 224)  \n",
    "\n",
    "        # Final fully connected layer that takes both image features and attributes\n",
    "        self.final_fc = nn.Linear(num_ftrs + 224, 2)  # Modify num_classes based on your classification problem\n",
    "\n",
    "    def forward(self, image, attributes):\n",
    "        # Get image features from the ResNet\n",
    "        img_features = self.base_model(image)\n",
    "\n",
    "        # Process attributes\n",
    "        attr_features = self.attr_fc(attributes)\n",
    "\n",
    "        # Concatenate image and attribute features\n",
    "        combined_features = torch.cat((img_features, attr_features), dim=1)\n",
    "\n",
    "        # Final classification layer\n",
    "        return self.final_fc(combined_features)\n",
    "\n",
    "# Update the model\n",
    "multi_attr_model = MultiInputResNet(base_model=model, num_attributes=num_attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a5e700fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, checkpoint_name=\"muti_attr_checkpoint.pt\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            checkpoint_name (str): Name of the checkpoint file. \n",
    "                            Default: \"checkpoint.pt\"\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "        self.checkpoint_name = checkpoint_name \n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Saves model when validation loss decreases.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.checkpoint_name) \n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "        \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, checkpoint_name):\n",
    "    # Initialize early stopping\n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True, checkpoint_name=checkpoint_name)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_pbar = tqdm(train_loader, unit=\"batch\")\n",
    "\n",
    "        for images, labels, attributes in train_pbar:\n",
    "            train_pbar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            images, labels, attributes = images.to(device), labels.to(device).float(), attributes.to(device).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, attributes)\n",
    "            \n",
    "            labels = labels.view(-1, 1) if labels.ndim == 1 else labels\n",
    "            \n",
    "            loss = criterion(outputs, labels)  \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            train_pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_pbar = tqdm(val_loader, unit=\"batch\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels, attributes in val_pbar:\n",
    "                val_pbar.set_description(f\"Val Epoch {epoch+1}/{num_epochs}\")\n",
    "                images, labels, attributes = images.to(device), labels.to(device).float(), attributes.to(device).float()\n",
    "                \n",
    "                outputs = model(images, attributes)\n",
    "                \n",
    "                labels = labels.view(-1, 1) if labels.ndim == 1 else labels\n",
    "                \n",
    "                loss = criterion(outputs, labels)  \n",
    "                val_running_loss += loss.item() * images.size(0)\n",
    "                val_pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} Train loss: {train_loss:.4f} Val loss: {val_loss:.4f}')\n",
    "\n",
    "        # Call early stopping\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    model.load_state_dict(torch.load(checkpoint_name))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "826781fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                                   | 0/5087 [00:04<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([32, 1])) must be the same as input size (torch.Size([32, 2]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Change this to your desired number of epochs\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Run the training function\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_attr_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/weights/multi_attr_checkpoint.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     34\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Save the trained model state\u001b[39;00m\n\u001b[1;32m     37\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(trained_model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/weights/multi_attr_model_final.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[104], line 72\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, checkpoint_name)\u001b[0m\n\u001b[1;32m     68\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images, attributes)\n\u001b[1;32m     70\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m labels\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m labels\n\u001b[0;32m---> 72\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     73\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     74\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/gender_classification/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gender_classification/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gender_classification/lib/python3.10/site-packages/torch/nn/modules/loss.py:725\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gender_classification/lib/python3.10/site-packages/torch/nn/functional.py:3193\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3190\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m-> 3193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([32, 1])) must be the same as input size (torch.Size([32, 2]))"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torchvision.models import resnet50\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming the `MultiInputResNet` class has been defined as discussed previously\n",
    "\n",
    "# Check if GPU is available and move the model to GPU\n",
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Assuming train_loader and val_loader have been defined and are ready to use\n",
    "# Define the loss function and optimizer\n",
    "criterion = BCEWithLogitsLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1, verbose=True)\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 10  # Change this to your desired number of epochs\n",
    "\n",
    "# Run the training function\n",
    "trained_model = train_model(\n",
    "    model=multi_attr_model, \n",
    "    train_loader=train_loader, \n",
    "    val_loader=val_loader, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer, \n",
    "    scheduler=scheduler, \n",
    "    num_epochs=num_epochs, \n",
    "    device=device, \n",
    "    checkpoint_name=\"/weights/multi_attr_checkpoint.pt\"\n",
    ")\n",
    "\n",
    "# Save the trained model state\n",
    "torch.save(trained_model.state_dict(), \"/weights/multi_attr_model_final.pt\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bca599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4444f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe6060b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
